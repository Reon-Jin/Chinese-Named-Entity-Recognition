from docopt import docopt
from vocab import Vocab
import time
import torch
import torch.nn as nn
from models.bilstm_crf import BiLSTMCRF
import utils
import random
from tqdm import tqdm
from models.transformer_crf import TransformerCRF
import gc

def get_args():
    """åˆ›å»ºå¹¶è¿”å›å‚æ•°å­—å…¸ï¼Œæ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°"""
    args = {
        'train': False,  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºè®­ç»ƒæ¨¡å¼
        'test': True,  # è®¾ç½®ä¸ºFalse
        'TRAIN': './data/NER-train-utf8.txt',  # è®­ç»ƒæ•°æ®è·¯å¾„
        'TEST': './data/NER-test-utf8.txt',  # æµ‹è¯•æ•°æ®è·¯å¾„
        'RESULT': './result.txt',  # ç»“æœä¿å­˜è·¯å¾„
        'SENT_VOCAB': './vocab/sent_vocab.json',  # å¥å­è¯å…¸è·¯å¾„
        'TAG_VOCAB': './vocab/tag_vocab.json',  # æ ‡ç­¾è¯å…¸è·¯å¾„
        'MODEL': './trained_model/T/model.pth',  # æ¨¡å‹è·¯å¾„
        '--dropout-rate': '0.3',
        '--embed-size': '256',
        '--hidden-size': '256',
        '--batch-size': '32',
        '--max-epoch': '100',
        '--clip_max_norm': '5.0',
        '--lr': '1e-3',
        '--log-every': '10',
        '--max-patience': '2',
        '--max-decay': '4',
        '--lr-decay': '0.5',
        '--model-save-path': './trained_model/T/model.pth',
        '--optimizer-save-path': './trained_model/T/optimizer.pth',
        '--cuda': True,
        '--debug-train': False,              # æ˜¯å¦åœ¨è®­ç»ƒæ—¶æ‰“å°é¢„æµ‹ï¼ˆé»˜è®¤ Trueï¼‰
        '--debug-train-samples': '2'  
    }
    return args

def train(args):
    """ Training BiLSTMCRF model
    Args:
        args: dict that contains options in command
    """
    sent_vocab = Vocab.load(args['SENT_VOCAB'])
    tag_vocab = Vocab.load(args['TAG_VOCAB'])
    train_data, dev_data = utils.generate_train_dev_dataset(args['TRAIN'], sent_vocab, tag_vocab)
    print('num of training examples: %d' % (len(train_data)))
    print('num of development examples: %d' % (len(dev_data)))

    max_epoch = int(args['--max-epoch'])
    model_save_path = args['--model-save-path']
    optimizer_save_path = args['--optimizer-save-path']
    min_dev_loss = float('inf')
    device = torch.device('cuda' if args['--cuda'] else 'cpu')
    patience, decay_num = 0, 0

    #model = BiLSTMCRF(sent_vocab, tag_vocab, float(args['--dropout-rate']), int(args['--embed-size']),int(args['--hidden-size'])).to(device)
    model = TransformerCRF.load(args['MODEL'], device)
    '''
    for name, param in model.named_parameters():
        if 'weight' in name:
            nn.init.normal_(param.data, 0, 0.01)
        else:
            nn.init.constant_(param.data, 0)
    '''
    optimizer = torch.optim.AdamW(model.parameters(), lr=float(args['--lr']))

    print('start training...')

    # è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    train_history = {
        'train_loss': [],
        'dev_loss': [],
        'learning_rate': []
    }

    debug_train = bool(args['--debug-train'])
    debug_train_samples = int(args['--debug-train-samples'])

    for epoch in range(max_epoch):
        # è®­ç»ƒé˜¶æ®µ - ä½¿ç”¨æ›´è¯¦ç»†çš„è¿›åº¦æ¡
        model.train()
        epoch_loss = 0
        total_samples = 0
        total_batches = 0

        # è®¡ç®—æ€»batchæ•°ç”¨äºè¿›åº¦æ¡
        total_batches_estimate = len(train_data) // int(args['--batch-size']) + 1

        # åˆ›å»ºæ›´è¯¦ç»†çš„è¿›åº¦æ¡
        train_iterator = utils.batch_iter(train_data, batch_size=int(args['--batch-size']))
        pbar = tqdm(train_iterator,
                    desc=f'ğŸš€ Epoch {epoch + 1}/{max_epoch}',
                    total=total_batches_estimate,
                    unit='batch',
                    bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}',
                    ncols=180)

        batch_start_time = time.time()

        for batch_idx, (raw_sentences, raw_tags) in enumerate(pbar):
            current_batch_size = len(raw_sentences)

            # pad inputs and tags (padded tensors on device)
            padded_sentences, sent_lengths = utils.pad(raw_sentences, sent_vocab[sent_vocab.PAD], device)
            padded_tags, _ = utils.pad(raw_tags, tag_vocab[tag_vocab.PAD], device)

            # back propagation
            optimizer.zero_grad()
            batch_loss = model(padded_sentences, padded_tags, sent_lengths)
            loss = batch_loss.mean()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(args['--clip_max_norm']))
            optimizer.step()

            batch_loss_value = batch_loss.mean().item()
            epoch_loss += batch_loss.sum().item()
            total_samples += current_batch_size
            total_batches += 1

            # ======== æ–°å¢ï¼šè®­ç»ƒæ—¶æŒ‰ batch æ‰“å°è‹¥å¹²æ ·æœ¬çš„é¢„æµ‹ä¿¡æ¯ï¼ˆå— debug æ§åˆ¶ï¼‰ ========
            if debug_train:
                # åœ¨è®­ç»ƒä¸­ä¸´æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼è¿›è¡Œé¢„æµ‹ï¼Œç„¶åæ¢å¤ train
                model.eval()
                with torch.no_grad():
                    try:
                        predicted_tags = model.predict(padded_sentences, sent_lengths)
                    except Exception as e:
                        predicted_tags = [[] for _ in range(current_batch_size)]
                        print(f"[WARN] model.predict failed during training debug: {e}")

                    n_print = min(debug_train_samples, current_batch_size)
                    for i in range(n_print):
                        # raw_sentences[i], raw_tags[i] æ˜¯åŸå§‹ index åˆ—è¡¨ï¼ˆå«é¦–å°¾æ ‡è®°ï¼‰ï¼Œpredicted_tags[i] æ˜¯å¯¹åº”çš„ id åˆ—è¡¨
                        sent_ids = raw_sentences[i]
                        true_ids = raw_tags[i]
                        pred_ids = predicted_tags[i] if i < len(predicted_tags) else []

                        # å»æ‰é¦–å°¾ï¼ˆè·Ÿæµ‹è¯•è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
                        sent_ids_trim = sent_ids[1:-1]
                        true_ids_trim = true_ids[1:-1]
                        # predicted å¯èƒ½é•¿åº¦å’Œ true ä¸å®Œå…¨ä¸€æ ·ï¼Œå°½é‡å¯¹é½
                        if len(pred_ids) >= 2:
                            pred_ids_trim = pred_ids[1:-1]
                        else:
                            pred_ids_trim = pred_ids

                        sent_words = [sent_vocab.id2word(x) for x in sent_ids_trim]
                        true_tags_words = [tag_vocab.id2word(x) for x in true_ids_trim]
                        pred_tags_words = [tag_vocab.id2word(x) for x in pred_ids_trim]

                        gold_entities = extract_entities(true_tags_words)
                        pred_entities = extract_entities(pred_tags_words)

                        print(f"[Train Debug] Epoch {epoch+1} Batch {batch_idx} Loss:{batch_loss_value:.4f} Sample:{i}")
                        print(" Sentence: ", " ".join(sent_words))
                        print(" True tags:", " ".join(true_tags_words))
                        print(" Pred tags:", " ".join(pred_tags_words))
                        print(" Gold entities:", gold_entities)
                        print(" Pred entities:", pred_entities)
                        print("-" * 40)
                model.train()
            # =====================================================================

            del padded_sentences, padded_tags, sent_lengths, batch_loss, loss
            torch.cuda.empty_cache() if args['--cuda'] else None
            gc.collect()
            # è®¡ç®—å¤„ç†é€Ÿåº¦
            batch_time = time.time() - batch_start_time
            samples_per_sec = current_batch_size / batch_time if batch_time > 0 else 0

            # æ›´æ–°è¿›åº¦æ¡æè¿° - æ›´è¯¦ç»†çš„ä¿¡æ¯
            avg_epoch_loss = epoch_loss / total_samples
            current_lr = optimizer.param_groups[0]['lr']

            pbar.set_postfix({
                'Batch_Loss': f'{batch_loss_value:.4f}',
                'Epoch_Loss': f'{avg_epoch_loss:.4f}',
                'LR': f'{current_lr:.2e}',
                'Samples/Sec': f'{samples_per_sec:.1f}',
                'Patience': f'{patience}/{args["--max-patience"]}'
            })

            batch_start_time = time.time()

        # è®¡ç®—epochå¹³å‡æŸå¤±
        epoch_avg_loss = epoch_loss / total_samples
        train_history['train_loss'].append(epoch_avg_loss)
        train_history['learning_rate'].append(optimizer.param_groups[0]['lr'])

        # æ¯ä¸ªepochç»“æŸåè¿›è¡ŒéªŒè¯
        print(f'\nğŸ“Š Epoch {epoch + 1} è®­ç»ƒå®Œæˆ, å¼€å§‹éªŒè¯...')
        print(f'è®­ç»ƒæŸå¤±: {epoch_avg_loss:.4f}')

        dev_loss = cal_dev_loss(model, dev_data, 64, sent_vocab, tag_vocab, device)
        train_history['dev_loss'].append(dev_loss)
        print("æœ¬è½®éªŒè¯æŸå¤±",dev_loss)
        print("ä¹‹å‰æœ€ä½³éªŒè¯æŸå¤±", min_dev_loss)
        if dev_loss < min_dev_loss * 0.98:
            improvement = min_dev_loss - dev_loss
            min_dev_loss = dev_loss
            model.save(model_save_path)
            torch.save(optimizer.state_dict(), optimizer_save_path)
            patience = 0
            print(f'ğŸ‰ æ¨¡å‹æœ‰æ”¹è¿›! æŸå¤±ä¸‹é™: {improvement:.4f}')
            print(f'ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}')
        else:
            patience += 1
            print(f'ğŸ˜ æš‚æ— æ”¹è¿›ï¼Œè€å¿ƒè®¡æ•°: {patience}/{args["--max-patience"]}')

            if patience == int(args['--max-patience']):
                decay_num += 1
                if decay_num == int(args['--max-decay']):
                    print('ğŸ›‘ æå‰åœæ­¢è§¦å‘! è®­ç»ƒç»“æŸã€‚')
                    break

                # å­¦ä¹ ç‡è¡°å‡
                old_lr = optimizer.param_groups[0]['lr']
                lr = old_lr * float(args['--lr-decay'])

                # åŠ è½½ä¹‹å‰ä¿å­˜çš„æœ€ä½³æ¨¡å‹
                print('ğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¡°å‡å­¦ä¹ ç‡...')

                model = BiLSTMCRF.load(model_save_path, device)
                #model = TransformerCRF.load(model_save_path,device)

                optimizer.load_state_dict(torch.load(optimizer_save_path))
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr
                patience = 0
                print(f'ğŸ“‰ å­¦ä¹ ç‡ä» {old_lr:.2e} è¡°å‡è‡³: {lr:.2e}')

        # è¾“å‡ºéªŒè¯ç»“æœ - æ›´ç›´è§‚çš„æ˜¾ç¤º
        print('-' * 70)
        print(f'âœ… éªŒè¯ç»“æœ - Epoch {epoch + 1}')
        print(f'   è®­ç»ƒæŸå¤±: {epoch_avg_loss:.4f}')
        print(f'   éªŒè¯æŸå¤±: {dev_loss:.4f}')
        print(f'   æœ€ä½³éªŒè¯æŸå¤±: {min_dev_loss:.4f}')
        print(f'   å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.2e}')
        print(f'   è€å¿ƒè®¡æ•°: {patience}/{args["--max-patience"]}')
        print(f'   è¡°å‡æ¬¡æ•°: {decay_num}/{args["--max-decay"]}')
        print('-' * 70)

        print('\n' + '=' * 70 + '\n')

    # è®­ç»ƒç»“æŸæ€»ç»“
    print('ğŸŠ è®­ç»ƒå®Œæˆ!')
    print(f'ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {model_save_path}')
    print(f'ğŸ“ˆ æœ€ç»ˆéªŒè¯æŸå¤±: {min_dev_loss:.4f}')
    print(f'ğŸ”„ æ€»å­¦ä¹ ç‡è¡°å‡æ¬¡æ•°: {decay_num}')

def extract_entities(tag_seq):
    """
    å°†BIOæ ‡ç­¾åºåˆ—è½¬ä¸ºå®ä½“ span åˆ—è¡¨
    tag_seq: ["B-ORG", "I-ORG", "N", "B-PER"...]
    è¿”å› [(start, end, type), ...]
    """
    entities = []
    start, ent_type = None, None

    for i, tag in enumerate(tag_seq):

        if tag.startswith('B-'):
            # è‹¥å‰ä¸€ä¸ªå®ä½“æœªç»“æŸï¼Œå…ˆå…³é—­
            if start is not None:
                entities.append((start, i - 1, ent_type))
            start = i
            ent_type = tag[2:]

        elif tag.startswith('I-'):
            # åŒä¸€å®ä½“ç»§ç»­
            continue

        else:  # N
            if start is not None:
                entities.append((start, i - 1, ent_type))
                start, ent_type = None, None

    if start is not None:
        entities.append((start, len(tag_seq) - 1, ent_type))

    return entities


def tst(args):
    """ Testing the model with P/R/F1 + æ¯ç±»å®ä½“çš„P/R/F1 """

    sent_vocab = Vocab.load(args['SENT_VOCAB'])
    tag_vocab = Vocab.load(args['TAG_VOCAB'])

    sentences, tags = utils.read_corpus(args['TEST'])
    sentences = utils.words2indices(sentences, sent_vocab)
    tags = utils.words2indices(tags, tag_vocab)
    test_data = list(zip(sentences, tags))
    print('num of test samples: %d' % (len(test_data)))

    device = torch.device('cuda' if args['--cuda'] else 'cpu')
    #model = BiLSTMCRF.load(args['MODEL'], device)

    model = TransformerCRF.load(args['MODEL'], device)

    print('start testing...')

    result_file = open(args['RESULT'], 'w')
    model.eval()

    # ==== æ€»æŒ‡æ ‡ ====
    total_gold = 0
    total_pred = 0
    total_correct = 0

    # ==== åˆ†ç±»åˆ«æŒ‡æ ‡ ====
    types = ["ORG", "LOC", "PER"]
    gold_per_type = {t: 0 for t in types}
    pred_per_type = {t: 0 for t in types}
    correct_per_type = {t: 0 for t in types}

    total_batches = len(test_data) // int(args['--batch-size']) + 1

    with torch.no_grad():
        test_iterator = utils.batch_iter(test_data, batch_size=int(args['--batch-size']), shuffle=False)

        for sentences, tags in tqdm(test_iterator, desc="ğŸ§ª æµ‹è¯•ä¸­",
                                    total=total_batches, unit='batch'):

            padded_sentences, sent_lengths = utils.pad(sentences, sent_vocab[sent_vocab.PAD], device)

            predicted_tags = model.predict(padded_sentences, sent_lengths)

            for sent, true_tags, pred_tags in zip(sentences, tags, predicted_tags):

                sent = sent[1:-1]
                true_tags = true_tags[1:-1]
                pred_tags = pred_tags[1:-1]

                # å†™ result.txt
                for tok, t, p in zip(sent, true_tags, pred_tags):
                    result_file.write(f"{sent_vocab.id2word(tok)} "
                                      f"{tag_vocab.id2word(t)} "
                                      f"{tag_vocab.id2word(p)}\n")
                result_file.write("\n")

                # BIO æ ‡ç­¾è½¬æ–‡å­—
                true_text = [tag_vocab.id2word(x) for x in true_tags]
                pred_text = [tag_vocab.id2word(x) for x in pred_tags]

                # N -> O
                true_text = [t for t in true_text]
                pred_text = [t for t in pred_text]

                gold_entities = extract_entities(true_text)
                pred_entities = extract_entities(pred_text)

                # -------------- æ–°å¢ï¼šåœ¨æ§åˆ¶å°æ‰“å°æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ --------------
                # æ‰“å°å¥å­ã€çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€å®ä½“åˆ—è¡¨
                sent_words = [sent_vocab.id2word(x) for x in sent]
                true_tags_words = [tag_vocab.id2word(x) for x in true_tags]
                pred_tags_words = [tag_vocab.id2word(x) for x in pred_tags]

                print("Sentence: ", " ".join(sent_words))
                print("True tags:", " ".join(true_tags_words))
                print("Pred tags:", " ".join(pred_tags_words))
                print("Gold entities:", gold_entities)
                print("Pred entities:", pred_entities)
                print("-" * 40)
                # -----------------------------------------------------------------

                total_gold += len(gold_entities)
                total_pred += len(pred_entities)

                # æŒ‰ç±»å‹ç»Ÿè®¡
                for (s, e, t) in gold_entities:
                    if t in types:
                        gold_per_type[t] += 1

                for (s, e, t) in pred_entities:
                    if t in types:
                        pred_per_type[t] += 1

                # ä¸¥æ ¼åŒ¹é…
                for ent in pred_entities:
                    if ent in gold_entities:
                        total_correct += 1
                        if ent[2] in types:
                            correct_per_type[ent[2]] += 1

    # ==== æ€»æŒ‡æ ‡ ====
    precision = total_correct / total_pred if total_pred else 0
    recall = total_correct / total_gold if total_gold else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0

    print("\nğŸ“Š Overall NER Results:")
    print(f"   Gold:      {total_gold}")
    print(f"   Predicted: {total_pred}")
    print(f"   Correct:   {total_correct}")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall:    {recall:.4f}")
    print(f"   F1:        {f1:.4f}\n")

    # ==== è¾“å‡ºæ¯ç±»å®ä½“çš„ç»“æœ ====
    print("ğŸ“Œ Per-Entity-Type Results:")
    print(f"{'Type':<6} {'P':<8} {'R':<8} {'F1':<8} {'Gold':<6} {'Pred':<6} {'Correct'}")
    print("-" * 60)

    for t in types:
        g = gold_per_type[t]
        p = pred_per_type[t]
        c = correct_per_type[t]

        P = c / p if p else 0
        R = c / g if g else 0
        F = 2 * P * R / (P + R) if (P + R) else 0

        print(f"{t:<6} {P:.4f}   {R:.4f}   {F:.4f}   {g:<6} {p:<6} {c}")

    print("\nâœ… æµ‹è¯•å®Œæˆï¼")




def cal_dev_loss(model, dev_data, batch_size, sent_vocab, tag_vocab, device):
    """ Calculate loss on the development data
    Args:
        model: the model being trained
        dev_data: development data
        batch_size: batch size
        sent_vocab: sentence vocab
        tag_vocab: tag vocab
        device: torch.device on which the model is trained
    Returns:
        the average loss on the dev data
    """
    is_training = model.training
    model.eval()
    loss, n_sentences = 0, 0

    # è®¡ç®—æ€»batchæ•°
    total_batches = len(dev_data) // batch_size + 1

    with torch.no_grad():
        dev_iterator = utils.batch_iter(dev_data, batch_size, shuffle=False)
        for sentences, tags in tqdm(dev_iterator,
                                    desc='ğŸ” éªŒè¯ä¸­',
                                    total=total_batches,
                                    leave=False,
                                    unit='batch'):
            sentences, sent_lengths = utils.pad(sentences, sent_vocab[sent_vocab.PAD], device)
            tags, _ = utils.pad(tags, tag_vocab[tag_vocab.PAD], device)

            batch_loss = model(sentences, tags, sent_lengths)
            loss += batch_loss.sum().item()
            n_sentences += len(sentences)

    model.train(is_training)
    return loss / n_sentences


def main():
    # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„å‚æ•°è·å–å‡½æ•°ï¼Œè€Œä¸æ˜¯docopt
    args = get_args()

    random.seed(0)
    torch.manual_seed(0)
    if args['--cuda']:
        torch.cuda.manual_seed(0)

    if args['train']:
        train(args)
    elif args['test']:
        tst(args)


if __name__ == '__main__':
    main()
